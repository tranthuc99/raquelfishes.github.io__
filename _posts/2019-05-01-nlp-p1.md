---
layout: post
title: NLP [P1] - Sequence Models và Recurrent Neural Network
date: 2019-05-01 12:00:20 +0700
description:  sequence models, rnn la gi, recurrent neural network la gi
img: 2019-05-01-nlp-p1/rnn.png # Add image post (optional)
image-dir: /assets/img/2019-05-01-nlp-p1
fig-caption: # Add figcaption (optional)
category: [Architectures and Algorithms]
tags: [Natural Language Processing]
permalink: /2019/05/01/nlp-p1
---

>Trong series này mình xin giới thiệu về cấu trúc mạng **RNN - Recurrent Neural Network**, một cấu trúc được xem là tiền đề để phát triển cho lĩnh vực **NLP - Natural Language Processing**. 

* TOC
{:toc}

## Sequence Models là gì ?
`Sequence Model` - Mô hình tuần tự là một trong những mô hình được dùng để xử lí những bài toán mà dữ liệu mang tính tuần tự, liên tục như:
* `Speech Recognition` - **Nhận dạng tiếng nói** : Từ dữ liệu âm thanh đầu vào, đưa ra đoạn nội dung dưới dạng text
* `Music generation` - **Sáng tác âm nhạc**: Từ dữ liệu từ hàng trăm, ngàn bài hát, tự sinh ra giai điệu bài hát mới.
* `Sentiment Classification` - **Phân loại ý kiến**: Phân loại ý kiến, cảm nghĩ từ nội dung bình luận, câu từ, lời nói
* `Machine translation` - **Dịch máy**: Tự động dịch nội dung từ ngôn ngữ này sang ngôn ngữ khác với ngữ cảnh chính xác.

Có thể nói, những dữ liệu mà mô hình này xử lí thường mang tính liên tục, tuần tự. Ví dụ như các dữ liệu về âm thanh, thông tin về tần số cũng như cường độ có phần liên quan tới nhau trong một khoảng thời gian nhất định.
## Tại sao không phải là một mô hình mạng bình thường ?
Để trả lời câu hỏi này, chúng ta cần nhìn vào bản chất của dữ liệu và quá trình xử lí của mạng Neural bình thường.
<p align="center"><img src="{{page.image-dir}}/neural-network.png"/></p>
### Độ dài dữ liệu khác nhau
Điều đầu tiên cần xét tới đó chính là việc dữ liệu đầu vào và đầu ra có thể có độ dài khác nhau. Có thể dễ thấy nhất là trong ví dụ về **`Dịch máy`**.

**I love my cat (4)** -> **Tôi yêu con mèo của tôi(5)**

**That cat looks very fat(5)** -> **Con mèo kia nhìn trông béo vãi (7)**

Có thể thấy mạng Neural bình thường không thể xử lí được việc dữ liệu đầu vào và đầu ra là khác nhau.
### Không chia sẻ được các đặc trưng học được tại các vị trí khác nhau của dữ liệu 
Điều tiếp theo có thể kể đến đó là mạng Neural bình thường không thể chia sẻ được các đặc tính học được tại các ví trí khác nhau của dữ liệu đầu vào.

**Con mèo béo là con mèo lười**

Trong câu trên, mạng Neural không thể hiểu được sự liên quan giữa `mèo béo` và `mèo lười` mà chỉ có thể biết được đâu là `mèo`
## Mạng RNN - Recurrent Neural Network 
Ý tưởng tiếp cận chính của mạng RNN chính là sử dụng các thông tin từ các dữ liệu trước đó để dự đoán tại bước tiếp theo. Ví dụ với câu "Con mèo rất thích ăn ___" thì khả năng từ còn thiếu là **cá** là khá cao. Vậy làm thế nào để RNN có thể sử dụng được thông tin trước đó, việc nó cần làm là cần ghi nhớ lại những từ (thông tin) trước đó.
<p align="center"><img src="{{page.image-dir}}/rnn.png"/></p>

Để có thể ghi nhớ lại những thông tin trước đó, nó sẽ được cấp một bộ nhớ **s**. Việc tính toán **s** được dựa trên dữ liệu đầu vào và bộ nhớ **s<sub>t-1</sub>** trước đó :
<p align="center"><img src="{{page.image-dir}}/state.png"/></p>
Kết quả đầu ra thu được sẽ được tính toán dựa trên trạng thái bộ nhớ **s** hiện tại :
<p align="center"><img src="{{page.image-dir}}/output.png"/></p>
**U**, **W**, **V** ở đây là weights và bias được share giữa các bước tính toán của cả mạng. **Activation function** được sử dụng để tính **s** thường là **tanH** hoặc **ReLU**.
Như vậy, mạng RNN có thể ghi nhớ thông tin ở các bước phía trước, tuy nhiên trên thực tế, RNN không thể ghi nhớ được chuỗi thông tin quá dài vì gặp một số vấn đề phát sinh như **Vanishing Gradient** và **Exploding Gradient** và tuỳ vào từng kiểu mạng RNN mà ở mỗi bước tính toán có thể có kết quả đầu ra hay không. Về việc này mình sẽ nói chi tiết trong những bài viết sau.
Nhớ để lại comment nếu có điều gì cần thảo luận nhé :D

**Bài viết được tham khảo và sử dụng hình ảnh từ:**
* [1. Recurrent Neural Networks Tutorial, Part 1 - Introduction to RNNs](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)
* [2. Artificial Neural Network](https://en.wikipedia.org/wiki/Artificial_neural_network)
* [3. RNN Lecture from deeplearning.ai](https://www.youtube.com/watch?v=efWlOCE_6HY&list=PL1w8k37X_6L_s4ncq-swTBvKDWnRSrinI&index=2)
