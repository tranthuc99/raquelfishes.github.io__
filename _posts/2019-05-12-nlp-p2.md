---
layout: post
title: NLP [P2] - Long Short-Term Memory
date: 2019-05-12 12:00:20 +0700
description: lstm la gi, tim hieu ve lstm, tim hieu ve long short term memory, lstm
img: 2019-05-12-nlp-p2/LSTM3-chain.png # Add image post (optional)
image-dir: /assets/img/2019-05-12-nlp-p2
fig-caption: # Add figcaption (optional)
category: [Architectures and Algorithms]
tags: [Natural Language Processing]
permalink: /2019/05/12/nlp-p2
---
TIếp túc series về NLP, hôm nay mình xin giới thiệu tới các bạn một cấu trúc mạng mới - Long Short-Term Memory (LSTM) được giới thiệu bởi **Hochreiter & Schmidhuber (1997)**. Cấu trúc mạng này có thể khắc phục vấn đề phụ thuộc xa của mạng RNN. Nếu chưa theo dõi về RNN - Recurrent Neural Network ở phần trước thì bạn có thể tham khảo thêm bài viết [NLP - Sequence Models và Recurrent Neural Network [P1]]({{site.url}}/nlp-p1)

* TOC
{:toc}

## Vấn đề phụ thuộc xa

Trong bài viết trước về RNN, mình có nhắc tới vấn đề **Vanishing Gradient** và **Exploding Gradient** mà mạng RNN gặp phải trong quá trình học tập khi độ dài của câu quá dài. Ví dụ:
* Con mèo thích ăn ***cá***

Ở trong câu này, việc dự đoán từ tiếp theo không quá khó, chúng ta không cần quá phụ thuộc vào ngữ cảnh nhiều mà có thể xác định từ ***cá*** ở đây. RNN có thể dễ dàng học và xử lí được trường hợp này. Tuy nhiên hãy xét ví dụ tiếp theo:
* Tôi sinh ra và lớn lên ở Việt Nam...... Tôi nói rất tốt tiếng ***Việt***

Vấn đề bắt đầu xuất hiện, để có thể dự đoán được từ "loại ngôn ngữ" ở đây, RNN cần quay ngược và tìm kiếm thông tin "Việt Nam" để có thể trả lời được. Tuy nhiên lúc này, khoảng cách đã trở nên quá xa và RNN trở nên không thể nhớ được những gì ở vị trí xa nữa.

## Long Short-Term Memory
<p align="center"><img src="{{page.image-dir}}/LSTM3-chain.png"/></p>
LSTM được thiết kế để giải quyết vấn đề nêu trên, nó có khả năng ghi nhớ được thông tin ở xa nhờ có các tầng **cổng quên**, **cổng vào**, **cổng ra**. Cùng đi sâu vào kiến trúc của LSTM để có thể hiểu rõ hơn về chức năng của các cổng này.
### Cấu trúc mạng
<p align="center"><img src="{{page.image-dir}}/LSTM3-C-line.png"/></p>
Thay vì việc tính toán và truyền **state** đơn thuần như RNN, LSTM có khả năng chọn lọc những thông tin phù hợp cần ghi nhớ dựa vào ngữ cảnh. Nó có thể thêm mới, bỏ bớt thông tin vào **cell** bằng các **gate**.
#### Cổng quên - Forget Gate
Cổng đầu tiên của LSTM giúp nó quyết định thông tin nào sẽ được rút ra từ cell. Bằng việc sử dụng **sigmoid layer** sẽ đưa ra kết quả trong khoảng từ **0** đến **1**. Điều này có nghĩa rằng nếu kết quả càng gần 0 thì có nghĩa là **đừng nhớ gì** và ngược lại **nhớ hoàn toàn** kết quả trạng thái phía trước. Đầu vào của cổng là kết quả đầu ra trước đó **h<sub>t-1</sub>** và dữ liệu vào **x<sub>t</sub>**
<p align="center"><img src="{{page.image-dir}}/LSTM3_forget.png"/></p>

#### Cổng vào - Input Gate
Cổng tiếp theo của LSTM giúp nó quyết định thông tin nào sẽ được lưu trữ vào trong cell. Việc này được thực hiện nhờ 2 phần chính.
1. **Input gate layer**: Dùng hàm sigmoid để quyết định thông tin nào chúng ta sẽ cập nhật, thêm mới. 
2. **Cell candidate**: Dùng hàm tanh để quyết định xem sẽ cập nhật như thế nào, bao nhiêu thông tin.
<p align="center"><img src="{{page.image-dir}}/LSTM3-input.png"/></p>

Sau khi đã tính toán, việc tiếp theo cần làm là cập nhật giá trị của cell - **C**
<p align="center"><img src="{{page.image-dir}}/LSTM3-cell.png"/></p>


#### Cổng ra - Output gate
Cuối cùng, LSTM tính toán xem kết quả đầu ra là gì. Kết quả sẽ được tính toán dựa trên trạng thái tế bào đã được lọc.
<p align="center"><img src="{{page.image-dir}}/LSTM3-output.png"/></p>
Đầu tiên, tính toán để xem phần nào của tế bào được tính toán để đữa ra kết quả bằng việc sử dụng hàm sigmoid giống ở **Input gate layer**. Sau đó nhân với tín hiệu đã đưa qua hàm Tanh của cell để trả về vector đầu ra **h<sub>t</sub>**.
## LSTM with Keras
Keras cung cấp cho ta layer LSTM để có thể triển khai mô hình giải quyết các bài toán NLP nhanh chóng.
```python
keras.layers.LSTM(units, 
		  activation='tanh',
                  recurrent_activation='hard_sigmoid',
                  use_bias=True, 
                  kernel_initializer='glorot_uniform',
                  recurrent_initializer='orthogonal',
                  bias_initializer='zeros', 
                  unit_forget_bias=True, 
                  kernel_regularizer=None, 
                  recurrent_regularizer=None, 
                  bias_regularizer=None, 
                  activity_regularizer=None, 
                  kernel_constraint=None, 
                  recurrent_constraint=None, 
                  bias_constraint=None, 
                  dropout=0.0,
                  recurrent_dropout=0.0,
                  implementation=1, 
                  return_sequences=False,
                  return_state=False, 
                  go_backwards=False, 
                  stateful=False,
                  unroll=False)
```
Một số param cơ bản:
* **units**: Số chiều của output - Chính là độ lớn của vector **h<sub>t</sub>** đầu ra, gần giống với lớp fully-connected 
* **keras.layers.Dense** gần cuối của CNN.
* **activation**: Hàm kích hoạt sử dụng ở **cell candidate layer** và cổng ra sử dụng để tính toán đầu ra từ giá trị của cell. Như ở trong mô hình cơ bản thì mặc định của nó là hàm Tanh.
* **recurrent_activation**: Hàm kích hoạt ở các cổng quên, vào, ra - ở đây mặc định là hàm sigmoid
* **return_sequences**: Boolean - Quyết định xem sẽ trả về đầu ra cuối cùng của mạng hoặc tất cả các đầu ra.
* **unroll**: Boolean - Nếu True thì mạng sẽ không duỗi, vòng lặp sẽ được sử dụng để tính toán các giá trị, Unroll có thể tăng tốc độ xử lí của RNN tuy nhiên sẽ chiếm dụng bộ nhớ lớn, nên option này chỉ phù hợp với các dữ liệu ngắn.

## Một vài kiến trúc khác của LSTM
Ngoài kiến trúc cơ bản ở trên, LSTM đã được nhiều tổ chức phát triển và đưa ra các phiên bản cái tiến khác nhau.
<p align="center"><img src="{{page.image-dir}}/LSTM_1.png"/></p>
<p align="center"><img src="{{page.image-dir}}/LSTM_2.png"/></p>
<p align="center"><img src="{{page.image-dir}}/LSTM_3.png"/></p>
Tuy kiến trúc, cách tính toán có vẻ khác nhau nhưng ý tưởng về việc sử dụng các cổng vẫn được giữ nguyên. Vừa rồi là những chia sẽ của mình về kiến trúc mạng LSTM - Long Short-Term Memory, có gì cần chia sẻ hãy để lại comment nhé :D 

**Bài viết được sử dụng hình ảnh và tham khảo từ:**
* [1. Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
* [2. Keras — Recurrent Layers](https://keras.io/layers/recurrent/)